{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion MNIST Deep Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObKzwoRmbqQdNraAQDpde/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kroush/Thinkful-Capstones/blob/main/Fashion_MNIST_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywy0lI3PBfuE"
      },
      "source": [
        "# Fashion MNIST\r\n",
        "Basics of Deep Learning and Articial Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcn9ujB8Bund"
      },
      "source": [
        "## Data download and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1AxfPykdb-w"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "from tensorflow.keras.datasets import fashion_mnist\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras import optimizers\r\n",
        "\r\n",
        "from IPython.display import clear_output\r\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60A6gV5Qor3i",
        "outputId": "64fa5844-47e4-4e3a-a9cc-26efb1027580"
      },
      "source": [
        "# load dataset\r\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n",
        "\r\n",
        "# loaded data summary\r\n",
        "print(f'Train: X= {X_train.shape}, Y={Y_train.shape}')\r\n",
        "print(f'Test: X= {X_test.shape}, Y={Y_test.shape}')\r\n",
        "\r\n",
        "# reshape, 2D -> 1D, each pixel is a feature, \r\n",
        "# normalize as max value is 255\r\n",
        "\r\n",
        "input_dim = 784  # 28*28\r\n",
        "output_dim = nb_classes = 10\r\n",
        "\r\n",
        "X_train = X_train.reshape(60000, input_dim)\r\n",
        "X_test = X_test.reshape(10000, input_dim)\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "X_train /= 255\r\n",
        "X_test /= 255\r\n",
        "\r\n",
        "# processed data summary\r\n",
        "print(f'Train: X= {X_train.shape}, Y={Y_train.shape}')\r\n",
        "print(f'Test: X= {X_test.shape}, Y={Y_test.shape}')\r\n",
        "\r\n",
        "# one-hot encode categories\r\n",
        "Y_train = to_categorical(Y_train, nb_classes)\r\n",
        "Y_test = to_categorical(Y_test, nb_classes)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Train: X= (60000, 28, 28), Y=(60000,)\n",
            "Test: X= (10000, 28, 28), Y=(10000,)\n",
            "Train: X= (60000, 784), Y=(60000,)\n",
            "Test: X= (10000, 784), Y=(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_e8lgnnqOG3"
      },
      "source": [
        "Here we can see that there are 60,000 examples in the training set and 10,000 in the test. Each is a 28X28 pixel image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgj8HwsOB0HM"
      },
      "source": [
        "## ANN Model Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr9ZqybG1FFw"
      },
      "source": [
        "def ann_model(layers, neurons, act, opt, loss, bs):\r\n",
        "  model = Sequential()\r\n",
        "  for i in range(1,layers):\r\n",
        "    model.add(Dense(neurons, activation=act, input_shape=(784,)))\r\n",
        "  model.add(Dense(10, activation='softmax', input_shape=(784,)))\r\n",
        "  model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\r\n",
        "\r\n",
        "  model.fit(X_train, Y_train, batch_size=bs, epochs=20, verbose=0)\r\n",
        "  \r\n",
        "  ann_model.test_score = model.evaluate(X_test, Y_test, verbose=0)\r\n",
        "  ann_model.train_score = model.evaluate(X_train, Y_train, verbose=0)\r\n",
        "  return "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYXrNuMw2D6z",
        "outputId": "ed1684fe-14fe-42f1-ea1f-b90ac04daa9d"
      },
      "source": [
        "# test function to validate performance\n",
        "\n",
        "ann_model(3, 64, 'relu', 'sgd', 'categorical_crossentropy', 8)\n",
        "print(ann_model.test_score[1], ann_model.train_score[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8810999989509583 0.9178500175476074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbPXJYYN9ppo"
      },
      "source": [
        "We're only going to vary the number of layers, number of neurons, activation functions of the layers (save the final one), and the batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCF5_z3N9x0E"
      },
      "source": [
        "layer_list = [3,4,5]\r\n",
        "neuron_list = [128,64,16,8]\r\n",
        "activation_list = ['sigmoid', 'tanh', 'relu']\r\n",
        "batch_list = [8,32,128]\r\n",
        "results = []\r\n",
        "count = 0\r\n",
        "\r\n",
        "for w in layer_list:\r\n",
        "    for x in neuron_list:\r\n",
        "        for y in activation_list:\r\n",
        "            for z in batch_list:\r\n",
        "                count+=1\r\n",
        "                print(f'{count}/{len(layer_list)*len(neuron_list)*len(activation_list)*len(batch_list)}')\r\n",
        "                \r\n",
        "                ann_model(w, x, y, 'sgd', 'categorical_crossentropy', z)\r\n",
        "                test_score = ann_model.test_score[1]\r\n",
        "                train_score = ann_model.train_score[1]\r\n",
        "                results.append((w,x,y,z,test_score,train_score))\r\n",
        "                clear_output()                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fTNwoNZoHCUU",
        "outputId": "7da8264a-7b5e-40d2-88ef-430a50d26e2b"
      },
      "source": [
        "results_df = pd.DataFrame(results, columns=('layers', 'neurons', 'activation_fxn', 'batch_size', 'test_score', 'train_score'))\r\n",
        "results_df.sort_values(by='test_score', ascending=False, inplace=True)\r\n",
        "results_df.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>layers</th>\n",
              "      <th>neurons</th>\n",
              "      <th>activation_fxn</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>test_score</th>\n",
              "      <th>train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>128</td>\n",
              "      <td>tanh</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8867</td>\n",
              "      <td>0.925317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>5</td>\n",
              "      <td>128</td>\n",
              "      <td>tanh</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8847</td>\n",
              "      <td>0.929017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>5</td>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8844</td>\n",
              "      <td>0.916367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>4</td>\n",
              "      <td>128</td>\n",
              "      <td>tanh</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8840</td>\n",
              "      <td>0.924900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>5</td>\n",
              "      <td>64</td>\n",
              "      <td>tanh</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8825</td>\n",
              "      <td>0.922417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>5</td>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8801</td>\n",
              "      <td>0.921633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>5</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8799</td>\n",
              "      <td>0.921517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>4</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8795</td>\n",
              "      <td>0.917567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>4</td>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8793</td>\n",
              "      <td>0.910550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>5</td>\n",
              "      <td>128</td>\n",
              "      <td>tanh</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8790</td>\n",
              "      <td>0.909033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>5</td>\n",
              "      <td>64</td>\n",
              "      <td>tanh</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8789</td>\n",
              "      <td>0.908350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8789</td>\n",
              "      <td>0.925450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>4</td>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8776</td>\n",
              "      <td>0.925933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>4</td>\n",
              "      <td>64</td>\n",
              "      <td>tanh</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8776</td>\n",
              "      <td>0.903617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8765</td>\n",
              "      <td>0.902350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>tanh</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8760</td>\n",
              "      <td>0.913700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8754</td>\n",
              "      <td>0.913667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>4</td>\n",
              "      <td>64</td>\n",
              "      <td>tanh</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8750</td>\n",
              "      <td>0.919317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>5</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8741</td>\n",
              "      <td>0.904433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>tanh</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8725</td>\n",
              "      <td>0.896167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>128</td>\n",
              "      <td>tanh</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8715</td>\n",
              "      <td>0.894367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8710</td>\n",
              "      <td>0.895183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>4</td>\n",
              "      <td>128</td>\n",
              "      <td>tanh</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8704</td>\n",
              "      <td>0.897617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>tanh</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8654</td>\n",
              "      <td>0.896333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>tanh</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8641</td>\n",
              "      <td>0.892250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>relu</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8637</td>\n",
              "      <td>0.889433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8625</td>\n",
              "      <td>0.881467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>tanh</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8624</td>\n",
              "      <td>0.890617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>tanh</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8611</td>\n",
              "      <td>0.887133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>4</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8610</td>\n",
              "      <td>0.887333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>128</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8594</td>\n",
              "      <td>0.878917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>4</td>\n",
              "      <td>64</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8591</td>\n",
              "      <td>0.881100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>relu</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.877817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>4</td>\n",
              "      <td>128</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8586</td>\n",
              "      <td>0.877950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>relu</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8577</td>\n",
              "      <td>0.888233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>tanh</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8572</td>\n",
              "      <td>0.889733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>tanh</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8569</td>\n",
              "      <td>0.882800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>5</td>\n",
              "      <td>128</td>\n",
              "      <td>tanh</td>\n",
              "      <td>128</td>\n",
              "      <td>0.8569</td>\n",
              "      <td>0.876583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4</td>\n",
              "      <td>128</td>\n",
              "      <td>tanh</td>\n",
              "      <td>128</td>\n",
              "      <td>0.8555</td>\n",
              "      <td>0.874617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>4</td>\n",
              "      <td>64</td>\n",
              "      <td>tanh</td>\n",
              "      <td>128</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.870650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>5</td>\n",
              "      <td>64</td>\n",
              "      <td>tanh</td>\n",
              "      <td>128</td>\n",
              "      <td>0.8534</td>\n",
              "      <td>0.874933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8528</td>\n",
              "      <td>0.874767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>relu</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8528</td>\n",
              "      <td>0.878617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>relu</td>\n",
              "      <td>32</td>\n",
              "      <td>0.8527</td>\n",
              "      <td>0.875150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>4</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>128</td>\n",
              "      <td>0.8525</td>\n",
              "      <td>0.867833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>128</td>\n",
              "      <td>0.8513</td>\n",
              "      <td>0.869717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>relu</td>\n",
              "      <td>8</td>\n",
              "      <td>0.8509</td>\n",
              "      <td>0.883967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>tanh</td>\n",
              "      <td>128</td>\n",
              "      <td>0.8494</td>\n",
              "      <td>0.866550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>128</td>\n",
              "      <td>tanh</td>\n",
              "      <td>128</td>\n",
              "      <td>0.8493</td>\n",
              "      <td>0.867417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>tanh</td>\n",
              "      <td>128</td>\n",
              "      <td>0.8475</td>\n",
              "      <td>0.866717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    layers  neurons activation_fxn  batch_size  test_score  train_score\n",
              "3        3      128           tanh           8      0.8867     0.925317\n",
              "75       5      128           tanh           8      0.8847     0.929017\n",
              "79       5      128           relu          32      0.8844     0.916367\n",
              "39       4      128           tanh           8      0.8840     0.924900\n",
              "84       5       64           tanh           8      0.8825     0.922417\n",
              "78       5      128           relu           8      0.8801     0.921633\n",
              "87       5       64           relu           8      0.8799     0.921517\n",
              "51       4       64           relu           8      0.8795     0.917567\n",
              "43       4      128           relu          32      0.8793     0.910550\n",
              "76       5      128           tanh          32      0.8790     0.909033\n",
              "85       5       64           tanh          32      0.8789     0.908350\n",
              "6        3      128           relu           8      0.8789     0.925450\n",
              "42       4      128           relu           8      0.8776     0.925933\n",
              "49       4       64           tanh          32      0.8776     0.903617\n",
              "7        3      128           relu          32      0.8765     0.902350\n",
              "12       3       64           tanh           8      0.8760     0.913700\n",
              "15       3       64           relu           8      0.8754     0.913667\n",
              "48       4       64           tanh           8      0.8750     0.919317\n",
              "88       5       64           relu          32      0.8741     0.904433\n",
              "13       3       64           tanh          32      0.8725     0.896167\n",
              "4        3      128           tanh          32      0.8715     0.894367\n",
              "16       3       64           relu          32      0.8710     0.895183\n",
              "40       4      128           tanh          32      0.8704     0.897617\n",
              "93       5       16           tanh           8      0.8654     0.896333\n",
              "94       5       16           tanh          32      0.8641     0.892250\n",
              "60       4       16           relu           8      0.8637     0.889433\n",
              "9        3       64        sigmoid           8      0.8625     0.881467\n",
              "57       4       16           tanh           8      0.8624     0.890617\n",
              "58       4       16           tanh          32      0.8611     0.887133\n",
              "52       4       64           relu          32      0.8610     0.887333\n",
              "0        3      128        sigmoid           8      0.8594     0.878917\n",
              "45       4       64        sigmoid           8      0.8591     0.881100\n",
              "25       3       16           relu          32      0.8588     0.877817\n",
              "36       4      128        sigmoid           8      0.8586     0.877950\n",
              "96       5       16           relu           8      0.8577     0.888233\n",
              "21       3       16           tanh           8      0.8572     0.889733\n",
              "22       3       16           tanh          32      0.8569     0.882800\n",
              "77       5      128           tanh         128      0.8569     0.876583\n",
              "41       4      128           tanh         128      0.8555     0.874617\n",
              "50       4       64           tanh         128      0.8537     0.870650\n",
              "86       5       64           tanh         128      0.8534     0.874933\n",
              "18       3       16        sigmoid           8      0.8528     0.874767\n",
              "61       4       16           relu          32      0.8528     0.878617\n",
              "97       5       16           relu          32      0.8527     0.875150\n",
              "53       4       64           relu         128      0.8525     0.867833\n",
              "8        3      128           relu         128      0.8513     0.869717\n",
              "24       3       16           relu           8      0.8509     0.883967\n",
              "14       3       64           tanh         128      0.8494     0.866550\n",
              "5        3      128           tanh         128      0.8493     0.867417\n",
              "95       5       16           tanh         128      0.8475     0.866717"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsOGZe19QzHU"
      },
      "source": [
        "ann_model(3, 128, 'tanh', 'sgd', 'categorical_crossentropy', 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "627fhs6DR7vH",
        "outputId": "d451f938-e3eb-4a07-f346-012ca170ee37"
      },
      "source": [
        "print(f'Best Model\\n Test Score: {ann_model.test_score[1]}\\n Training Score:{ann_model.train_score[1]}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Model\n",
            " Test Score: 0.8848999738693237\n",
            " Training Score:0.9212499856948853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjo7tHRlRH5c"
      },
      "source": [
        "Here, I've displayed the top 50 models out of 108. In general, 'tanh' and 'relu' activators and the larger numbers of neurons (128 and 64) gave the best results. Other hyperparameters seem well mixed in the top 50 results. The training score is generally 0.2 - 0.4 higher than the test score, showing that there are quite a few good model options here to continue to fine tune with minimal overfitting. To continue optimization efforts, I'd likely start with the highest rank 'relu' model. With a mini batch size of 32, I have less concerns of overfitting than the other models with batch sizes of 8. The best performing model in both the test and training set is shown above."
      ]
    }
  ]
}